{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc1_'></a>[TP1 - Clasificación de Textos en 20 Newsgroups](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- [TP1 - Clasificación de Textos en 20 Newsgroups](#toc1_)    \n",
        "  - [Introducción](#toc1_1_)    \n",
        "    - [Consigna](#toc1_1_1_)    \n",
        "    - [Integrantes del grupo](#toc1_1_2_)    \n",
        "  - [Resolución](#toc1_2_)    \n",
        "    - [EDA](#toc1_2_1_)    \n",
        "      - [Balance de clases](#toc1_2_1_1_)    \n",
        "      - [**Preprocessing de la Data**](#toc1_2_1_2_)    \n",
        "    - [**Archivos**](#toc1_2_2_)    \n",
        "  - [**Gráficos (EDA)**](#toc1_3_)    \n",
        "  - [**Pipeline**](#toc1_4_)    \n",
        "  - [**Results**](#toc1_5_)    \n",
        "    - [**Train Scores**](#toc1_5_1_)    \n",
        "    - [**Test Scores**](#toc1_5_2_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_1_'></a>[Introducción](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_1_1_'></a>[Consigna](#toc0_)\n",
        "\n",
        "Se trabajará con los datos de 20 Newsgroups.\n",
        "\n",
        "Se pide:\n",
        "\n",
        "* Realizar un EDA (análisis exploratorio de datos) del dataset, entre otros: frecuencia de palabras, distribución de clases, longitud de los documentos.\n",
        "* Entrenar modelos de clasificación del tipo Naïve Bayes Multinomial y Regresión Logística. Realizar para ellos una búsqueda de hiperparámetros optimizando la métrica del desempeño de este problema. Separar adecuadamente los datos de validación de los de entrenamiento. Reportar métricas de desempeño en todos los casos.\n",
        "* Para el mejor de los modelos encontrados de cada tipo (Naïve Bayes y Regresión logística), reportar su métrica de desempeño en el conjunto de datos de test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_1_2_'></a>[Integrantes del grupo](#toc0_)\n",
        "\n",
        "* CILFONE ARGIBAY, Juan Pablo\n",
        "  * 62074\n",
        "  * [jcilfoneargibay@itba.edu.ar](mailto:jcilfoneargibay@itba.edu.ar)\n",
        "* HEIR, Alejandro Nahuel\n",
        "  * 62496\n",
        "  * [aheir@itba.edu.ar](mailto:aheir@itba.edu.ar)\n",
        "\n",
        "\n",
        "*22.67 - Redes Neuronales*\n",
        "\n",
        "*ITBA - Ingeniería Electrónica - Orientación Procesamiento de Señales - 4to año 2do cuatrimestre*\n",
        "\n",
        "*Abril - Mayo 2024*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_2_'></a>[Resolución](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title **20 News Groups**\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
        "twenty_test = fetch_20newsgroups(subset=\"test\", shuffle=True, remove=(\"headers\", \"footers\", \"quotes\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_2_1_'></a>[EDA](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5dW-2UAbZ7M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "class_balance = np.zeros(len(twenty_train[\"target_names\"]))\n",
        "for i in range(len(twenty_train[\"target\"])):\n",
        "  class_balance[twenty_train[\"target\"][i]] += 1\n",
        "\n",
        "class_balance_probs = class_balance / len(twenty_train[\"target\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(class_balance)    # Arrays con el balance de clases\n",
        "print(class_balance_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc1_2_1_1_'></a>[Balance de clases](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "5EaEIvg_fk_D",
        "outputId": "2974e92f-cc85-4223-bd1e-a2d4a2bfd64a"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(class_balance_probs, labels=[\"{0} ({1})\".format(twenty_train[\"target_names\"][i], class_balance[i]) for i in range(len(twenty_train[\"target_names\"]))], autopct=\"%1.1f%%\", startangle=90)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrE0wez5xrWs",
        "outputId": "1b6d70d3-6804-4069-a96a-ff0bffc0ecba"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# **Número de clases**\n",
        "num_classes = len(np.unique(twenty_train.target))\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# **Tamaño total del corpus**\n",
        "total_corpus_size = np.sum(class_balance)\n",
        "print(f\"Total corpus size: {total_corpus_size}\\n\")\n",
        "\n",
        "# **Número de textos por clase**\n",
        "for i, class_name in enumerate(twenty_train.target_names):\n",
        "  print(f\"Class '{class_name}': {class_balance[i]} texts\")\n",
        "\n",
        "# **Longitudes de los textos**\n",
        "text_lengths = [len(text.split()) for text in twenty_train.data]\n",
        "print(f\"\\nMinimum text length: {min(text_lengths)}\")\n",
        "print(f\"Maximum text length: {max(text_lengths)}\")\n",
        "print(f\"Average text length: {np.mean(text_lengths)}\")\n",
        "print(f\"Median text length: {np.median(text_lengths)}\\n\")\n",
        "\n",
        "# **Estadísticas de palabras**\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "X_train = vectorizer.fit_transform(twenty_train.data)\n",
        "\n",
        "unique_words_count = len(vectorizer.vocabulary_)\n",
        "print(f\"Number of unique words: {unique_words_count}\\n\")\n",
        "\n",
        "word_counts = np.sum(X_train.toarray(), axis=0)\n",
        "\n",
        "most_frequent_words = np.argsort(word_counts)[::-1][:10]\n",
        "for i in most_frequent_words:\n",
        "  print(f\"Most frequent word: '{vectorizer.get_feature_names_out()[i]}', count: {word_counts[i]}\")\n",
        "\n",
        "least_frequent_words = np.argsort(word_counts)[:10]\n",
        "for i in least_frequent_words:\n",
        "  print(f\"Least frequent word: '{vectorizer.get_feature_names_out()[i]}', count: {word_counts[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# **Plot de las longitudes de los textos**\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(text_lengths, bins=50)\n",
        "plt.xlabel(\"Text Length\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale('log')\n",
        "plt.title(\"Distribution of Text Lengths\")\n",
        "plt.show()\n",
        "\n",
        "# **Plot de las palabras más frecuentes**\n",
        "most_frequent_words = np.argsort(word_counts)[::-1][:10]\n",
        "most_frequent_words_names = [vectorizer.get_feature_names_out()[i] for i in most_frequent_words]\n",
        "most_frequent_words_counts = word_counts[most_frequent_words]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(most_frequent_words_names, most_frequent_words_counts)\n",
        "plt.xlabel(\"Word\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale('log')\n",
        "plt.title(\"Most Frequent Words\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(word_counts, bins=50, log=True)\n",
        "plt.xlabel(\"Word Frequency\")\n",
        "plt.ylabel(\"Number of Words\")\n",
        "plt.title(\"Distribution of Word Frequencies\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc1_2_1_2_'></a>[**Preprocessing de la Data**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OklwS26IPkW7",
        "outputId": "5fbd9885-8e38-44e5-fe64-204959477fd5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "706O8tMKPwxf"
      },
      "outputs": [],
      "source": [
        "def preprocess_article(article):\n",
        "    tok = word_tokenize(article)\n",
        "    lem = [lemmatizer.lemmatize(x, pos='v') for x in tok]\n",
        "    stop = [x for x in lem if x not in stopwords.words('english')]\n",
        "    stem = [stemmer.stem(x) for x in stop]\n",
        "    alpha = [x for x in stem if x.isalpha()]\n",
        "    return \" \".join(alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jnAaJbFP70C"
      },
      "source": [
        "Filtrado del Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSFd5hDkPy5j",
        "outputId": "0797d556-a35f-49e9-8f94-92bcef6ada0b"
      },
      "outputs": [],
      "source": [
        "train_filt=list()\n",
        "for i in tqdm(range(len(twenty_train.data))):\n",
        "    art = twenty_train.data[i]\n",
        "    filtered_article = preprocess_article(art)\n",
        "    train_filt.append(filtered_article)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fT12F3BRvHq"
      },
      "source": [
        "Filtrado del Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_filt=list()\n",
        "for i in tqdm(range(len(twenty_test.data))):\n",
        "    art = twenty_test.data[i]\n",
        "    filtered_article = preprocess_article(art)\n",
        "    test_filt.append(filtered_article)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_2_2_'></a>[**Archivos**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0l2powuzqI-"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZnLCLHHQSZe"
      },
      "outputs": [],
      "source": [
        "#Salvado del procesamiento a disco:\n",
        "# CUIDADO: AL CORRER SE BORRA/SOBREESCRIBE EL ARCHIVO 'art_filt.txt'\n",
        "with open('art_filt_train.txt', 'wb') as fp:\n",
        "    pickle.dump(train_filt, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('art_filt_test.txt', 'wb') as fp:\n",
        "    pickle.dump(test_filt, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_3_'></a>[**Gráficos (EDA)**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVGYBCaApWp2"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open ('art_filt_train.txt', 'rb') as fp:\n",
        "  train_data = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4suMHK0bez0_"
      },
      "outputs": [],
      "source": [
        "with open ('art_filt_test.txt', 'rb') as fp:\n",
        "  test_data = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20hfWF-L1gmR",
        "outputId": "13488d6a-5ed6-453c-b9ce-5a9ca64acafc"
      },
      "outputs": [],
      "source": [
        "# **Longitudes de los textos en el nuevo dataset**\n",
        "text_lengths = [len(text.split()) for text in train_data]\n",
        "print(f\"Minimum text length: {min(text_lengths)}\")\n",
        "print(f\"Maximum text length: {max(text_lengths)}\")\n",
        "print(f\"Average text length: {np.mean(text_lengths)}\")\n",
        "print(f\"Median text length: {np.median(text_lengths)}\\n\")\n",
        "\n",
        "# **Estadísticas de palabras**\n",
        "vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "X_train = vectorizer.fit_transform(train_data)\n",
        "\n",
        "unique_words_count = len(vectorizer.vocabulary_)\n",
        "print(f\"Number of unique words: {unique_words_count}\\n\")\n",
        "\n",
        "word_counts = np.sum(X_train.toarray(), axis=0)\n",
        "\n",
        "most_frequent_words = np.argsort(word_counts)[::-1][:20]\n",
        "for i in most_frequent_words:\n",
        "  print(f\"Most frequent word: '{vectorizer.get_feature_names_out()[i]}', count: {word_counts[i]}\")\n",
        "\n",
        "least_frequent_words = np.argsort(word_counts)[:10]\n",
        "for i in least_frequent_words:\n",
        "  print(f\"Least frequent word: '{vectorizer.get_feature_names_out()[i]}', count: {word_counts[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n5-kEbwnSlph",
        "outputId": "901ab340-e349-48ba-94e1-9691ad1454ea"
      },
      "outputs": [],
      "source": [
        "# **Gráfico de distribución de longitudes de textos**\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(text_lengths, bins=30)\n",
        "plt.xlabel(\"Text Length\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale('log')\n",
        "plt.title(\"Distribution of Text Lengths in the New Dataset\")\n",
        "plt.show()\n",
        "\n",
        "# **Gráfico de las palabras más frecuentes**\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(vectorizer.get_feature_names_out()[most_frequent_words], word_counts[most_frequent_words])\n",
        "plt.xlabel(\"Word\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Most Frequent Words in the New Dataset\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(word_counts, bins=50, log=True)\n",
        "plt.xlabel(\"Word Frequency\")\n",
        "plt.ylabel(\"Number of Words\")\n",
        "plt.title(\"Distribution of Word Frequencies\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVgZywe0vYJ1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define the range of max_df values\n",
        "max_df_values = np.arange(0.01, 0.06,0.01)\n",
        "\n",
        "# Initialize a list to store vocabulary sizes for each max_df value\n",
        "vocab_sizes_list = []\n",
        "\n",
        "# Iterate over max_df values\n",
        "for max_df in max_df_values:\n",
        "    vocab_sizes = []\n",
        "    for min_df in range(8, 15):\n",
        "        vectorizer = CountVectorizer(max_df=max_df, min_df=min_df, ngram_range=(1, 2))\n",
        "        X = vectorizer.fit_transform(train_data)\n",
        "        vocab_sizes.append(len(vectorizer.vocabulary_))\n",
        "    vocab_sizes_list.append(vocab_sizes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "-HxsV_KD-R5_",
        "outputId": "e08f464f-e530-4725-8f67-27078c80c83d"
      },
      "outputs": [],
      "source": [
        "# Plot the vocabulary size vs min_df for each max_df value\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, max_df in enumerate(max_df_values):\n",
        "    plt.plot(range(8, 15), vocab_sizes_list[i], label=f'max_df={max_df}')\n",
        "\n",
        "plt.xlabel(\"min_df\")\n",
        "plt.ylabel(\"Vocabulary size\")\n",
        "plt.title(\"Vocabulary Size vs Min_df for Different Max_df Values\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltcMBrNpDasx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "vocab_sizes = []\n",
        "for max_df in np.arange(9/11314, 0.2, 0.05):\n",
        "  vectorizer = CountVectorizer(max_df=max_df,min_df=8, ngram_range=(1, 2))\n",
        "  X = vectorizer.fit_transform(train_data)\n",
        "  vocab_sizes.append(vectorizer.vocabulary_.__len__())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the vocabulary size vs max_df\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(np.arange(9/11314,0.2,0.05), vocab_sizes)\n",
        "plt.xlabel(\"max_df\")\n",
        "plt.ylabel(\"Vocabulary size\")\n",
        "plt.title(\"Vocabulary Size vs Max_df\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_4_'></a>[**Pipeline**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nCSQvBK5Fgs"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from time import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "3zPY3oduRbA7",
        "outputId": "5bdaadc8-5cba-4166-dc52-4dd2805c64d0"
      },
      "outputs": [],
      "source": [
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    #('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "\n",
        "parameter_grid = {\n",
        "    \"vect__max_df\": np.linspace(0.02,.1,5),\n",
        "    \"vect__min_df\": (8,10,12),\n",
        "    #\"tfidf__use_idf\": (True, False),\n",
        "    \"vect__ngram_range\": ((1, 2),),  # unigrams or bigrams\n",
        "    \"clf__alpha\": np.logspace(-6,-2,5)\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "      grid_search = GridSearchCV(pipeline, parameter_grid, n_jobs=-1, verbose=10,cv=2)\n",
        "\n",
        "      print(\"Performing grid search...\")\n",
        "      print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
        "      print(\"parameters:\")\n",
        "      print(parameter_grid)\n",
        "      t0 = time()\n",
        "      grid_search.fit(train_data,twenty_train['target'])\n",
        "      print(\"done in %0.3fs\" % (time() - t0))\n",
        "      print()\n",
        "\n",
        "      print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "      print(\"Best parameters set:\")\n",
        "      best_parameters = grid_search.best_estimator_.get_params()\n",
        "      for param_name in sorted(parameter_grid.keys()):\n",
        "          print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('grid.txt', 'wb') as fp:\n",
        "    pickle.dump(grid_search, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_5_'></a>[**Results**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rbSFqIS2ZDO",
        "outputId": "391aae3b-fc58-40af-d741-e460c13e5872"
      },
      "outputs": [],
      "source": [
        "# @title **Grid Search**\n",
        "with open('grid.txt', 'rb') as fp:\n",
        "  grid_search_data = pickle.load(fp)\n",
        "\n",
        "print(\"Grid scores:\")\n",
        "for mean, params in zip(grid_search_data.cv_results_['mean_test_score'],  grid_search_data.cv_results_['params']):\n",
        "    print(\"%0.3f for %r\" % (mean, params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ### **Ejecución de un Naive Bayes \"Normal\" con parámetros óptimos**\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "vectorizer = CountVectorizer(max_df=0.1,min_df=8, ngram_range=(1, 2))\n",
        "X_train = vectorizer.fit_transform(train_data)\n",
        "clf = MultinomialNB(alpha=0.01)\n",
        "clf.fit(X_train, twenty_train[\"target\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_5_1_'></a>[**Train Scores**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ1iSEb5AD5R",
        "outputId": "c418194d-e77c-4a99-8237-6fa686b62b13"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "acc=sum(np.array(clf.predict(X_train))==np.array(twenty_train[\"target\"]))/len(twenty_train[\"target\"])*100\n",
        "print(\"El porcentaje de artículos clasificados correctamente es: {}%\".format(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbjeuD7vQHaX",
        "outputId": "fee02a9b-c5f0-48c8-84a7-87fe49ee828f"
      },
      "outputs": [],
      "source": [
        "acc=sum(np.array(grid_search_data.predict(train_data))==np.array(twenty_train[\"target\"]))/len(twenty_train[\"target\"])*100\n",
        "print(\"El porcentaje de artículos clasificados correctamente es: {}%\".format(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc = np.mean(clf.predict(X_train) == twenty_train.target) * 100\n",
        "print(\"El porcentaje de artículos clasificados correctamente es: {}%\".format(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_5_2_'></a>[**Test Scores**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHD5ar1vqsEW",
        "outputId": "676b2a12-394d-4f8b-ce3f-499a0ebf756e"
      },
      "outputs": [],
      "source": [
        "porc=sum(np.array(grid_search_data.predict(test_data))==np.array(twenty_test[\"target\"]))/len(twenty_test[\"target\"])*100\n",
        "print(\"El porcentaje de artículos clasificados correctamente es: {}%\".format(porc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXEGIsklq-wA",
        "outputId": "fbc5b879-d25e-458f-c840-e94766e46a5a"
      },
      "outputs": [],
      "source": [
        "X_test = vectorizer.transform(test_data)\n",
        "porc=sum(np.array(clf.predict(X_test))==np.array(twenty_test[\"target\"]))/len(twenty_test[\"target\"])*100\n",
        "print(\"El porcentaje de artículos clasificados correctamente es: {}%\".format(porc))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
